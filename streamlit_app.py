# -*- coding: utf-8 -*-
"""research_agent_databricks

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18RUkf8IpHVSJF-rDh8cOj0QJ6UwQonfh

# Building a Research Agent with Databricks

In this notebook we show you how to build a complete agent reasoning loop. Instead of tool calling in a single-shot setting, an agent is able to reason over tools in a multiple-steps. This necessitates that the agent can maintain state across the loop.

We will use our `FunctionCallingAgent` implementation, which is an agent that natively integrates with the function calling capabilities of LLMs.

### Setup
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install llama-index==0.10.28
# %pip install llama-index-llms-databricks
# %pip install llama-index-embeddings-huggingface
# %pip install llama-parse

import os
os.environ["LLAMA_CLOUD_API_KEY"] = "llx-..."

import nest_asyncio
nest_asyncio.apply()

# databricks api key
api_key = ""

from llama_index.llms.databricks import Databricks
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core import Settings

embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")
llm = Databricks(
    model="databricks-meta-llama-3-70b-instruct",
    api_key=api_key,
    api_base="https://<cluster_id>.cloud.databricks.com/serving-endpoints",
)

Settings.llm = llm
Settings.embed_model = embed_model

import nest_asyncio
nest_asyncio.apply()

"""## Download ~3 ICLR 2024 papers, use LlamaParse

Let's parse 3 ICLR 2024 research papers using LlamaParse.
"""

urls = [
    "https://openreview.net/pdf?id=VtmBAGCN7o",
    "https://openreview.net/pdf?id=6PmJoRfdaK",
    "https://openreview.net/pdf?id=hSyW5go0v8",
]

papers = [
    "metagpt.pdf",
    "longlora.pdf",
    "selfrag.pdf",
]

for url, paper in zip(urls, papers):
    !wget "{url}" -O "{paper}"

from llama_parse import LlamaParse

from llama_index.core.schema import Document
def _load_data(file_path: str) -> Document:
    parser = LlamaParse(result_type="text")
    json_objs = parser.get_json_result(file_path)
    json_list = json_objs[0]["pages"]
    docs = []
    for item in json_list:
        doc = Document(
            text=item["text"], metadata={"page_label": item["page"]}
        )
        docs.append(doc)
    return docs

"""### Convert papers to Tools"""

# TODO: abstract all of this into a function that takes in a PDF file name

from llama_index.core import VectorStoreIndex, SummaryIndex
from llama_parse import LlamaParse
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.tools import FunctionTool, QueryEngineTool
from llama_index.core.vector_stores import MetadataFilters, FilterCondition
from typing import List, Optional


def get_doc_tools(
    file_path: str,
    name: str,
) -> str:
    """Get vector query and summary query tools from a document."""

    # load documents
    # documents = LlamaParse(result_type="text").load_data(file_path)
    documents = _load_data(file_path)
    splitter = SentenceSplitter(chunk_size=1024)
    nodes = splitter.get_nodes_from_documents(documents)
    vector_index = VectorStoreIndex(nodes)

    def vector_query(
        query: str,
        page_numbers: Optional[List[int]] = None
    ) -> str:
        """Use to answer questions over a given paper.

        Useful if you have specific questions over the paper.
        Always leave page_numbers as None UNLESS there is a specific page you want to search for.

        Args:
            query (str): the string query to be embedded.
            page_numbers (Optional[List[int]]): Filter by set of pages. Leave as NONE
                if we want to perform a vector search
                over all pages. Otherwise, filter by the set of specified pages.

        """

        page_numbers = page_numbers or []
        metadata_dicts = [
            {"key": "page_label", "value": p} for p in page_numbers
        ]

        query_engine = vector_index.as_query_engine(
            similarity_top_k=2,
            filters=MetadataFilters.from_dicts(
                metadata_dicts,
                condition=FilterCondition.OR
            )
        )
        response = query_engine.query(query)
        return response


    vector_query_tool = FunctionTool.from_defaults(
        name=f"vector_tool_{name}",
        fn=vector_query
    )

    summary_index = SummaryIndex(nodes)
    summary_query_engine = summary_index.as_query_engine(
        response_mode="tree_summarize",
        use_async=True,
    )
    summary_tool = QueryEngineTool.from_defaults(
        name=f"summary_tool_{name}",
        query_engine=summary_query_engine,
        description=(
            f"Useful for summarization questions related to {name}"
        ),
    )

    return vector_query_tool, summary_tool

from pathlib import Path

paper_to_tools_dict = {}
for paper in papers:
    print(f"Getting tools for paper: {paper}")
    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)
    paper_to_tools_dict[paper] = [vector_tool, summary_tool]

"""## Setup an agent over 3 papers

We now setup our function calling agent over 3 papers. We do this by combining the vector/summary tools for each document into a list and passing it to the agent.
"""

initial_papers = ["metagpt.pdf", "selfrag.pdf", "longlora.pdf"]
initial_tools = [t for paper in initial_papers for t in paper_to_tools_dict[paper]]

# tmp = paper_to_tools_dict["selfrag.pdf"][1]("summary")
# print(str(tmp))

from llama_index.core.agent import ReActAgentWorker
from llama_index.core.agent import AgentRunner

agent_worker = ReActAgentWorker.from_tools(
    initial_tools,
    # llm=llm,
    verbose=True
)
agent = AgentRunner(agent_worker)

"""We first query the agent."""

response = agent.query(
    "Tell me about the evaluation dataset used in Self-RAG, and then tell me about the evaluation results"
)

print(str(response))

response = agent.query("What are the MetaGPT comparisons with ChatDev described on page 8 of the MetaGPT paper?")
print(str(response))

response = agent.query(
    "Compare the complexity of the approaches in Self-RAG and MetaGPT. Which approach uses more tokens?"
)

print(str(response))

